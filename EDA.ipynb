{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmlLrJY1zPqOPAE8jcigLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hojuna/Black-box_Optimization_dacon/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1\n",
        "0.   y값 30이하 쳐내기\n",
        "1.   로그 변한\n",
        "2.   standard\n",
        "3.   PCA만들\n",
        "\n"
      ],
      "metadata": {
        "id": "rGS4QOL-flaD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62TfPlZQft6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O1NSoiE5x58y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b5a0f3-e5ef-4885-fae2-52c21834e371"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/dacon/2024_8_data/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/dacon/2024_8_data/test.csv')"
      ],
      "metadata": {
        "id": "myZ7BBXx9ne_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaned = train_df[train_df['y'] > 50]\n",
        "\n",
        "# data_cleaned = train_df.copy()"
      ],
      "metadata": {
        "id": "lHauWiY7gZrM"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 예시 데이터 로드\n",
        "# data = pd.read_csv('your_data.csv')\n",
        "\n",
        "def columns_to_log_transform(df):\n",
        "  # 1. 로그 변환 (음수 값 처리 후)\n",
        "  columns_to_log_transform = ['x_0', 'x_1', 'x_2', 'x_5', 'x_6', 'x_7']\n",
        "\n",
        "  for col in columns_to_log_transform:\n",
        "      min_value = df[col].min()\n",
        "      # 최소값 보정 후 로그 변환\n",
        "      df[f'{col}'] = np.log(df[col] - min_value + 1)\n",
        "  return df\n",
        "\n",
        "# 로그 변환 적용\n",
        "data_cleaned=columns_to_log_transform(data_cleaned)\n",
        "test_df=columns_to_log_transform(test_df)\n",
        "\n",
        "data_cleaned.info()\n",
        "# 2. 로그 변환이 끝난 후, 스케일링 적용 (StandardScaler 사용)\n",
        "x_scaler = StandardScaler()\n",
        "colums=data_cleaned.iloc[:,1:-1].columns\n",
        "\n",
        "data_scaled = x_scaler.fit_transform(data_cleaned[colums])\n",
        "test_scaled = x_scaler.transform(test_df[colums])\n",
        "\n",
        "\n",
        "# 스케일된 데이터를 다시 데이터프레임으로 변환\n",
        "# scaled_df = pd.DataFrame(data_scaled)\n",
        "data_cleaned[colums]=data_scaled\n",
        "test_df[colums]=test_scaled\n",
        "\n",
        "# print(scaled_df.head())\n",
        "\n",
        "data_cleaned.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct-7tI0eg6bp",
        "outputId": "c1dfdddb-3b95-4bdc-d460-01a46426c97f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 40116 entries, 0 to 40117\n",
            "Data columns (total 13 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   ID      40116 non-null  object \n",
            " 1   x_0     40116 non-null  float64\n",
            " 2   x_1     40116 non-null  float64\n",
            " 3   x_2     40116 non-null  float64\n",
            " 4   x_3     40116 non-null  float64\n",
            " 5   x_4     40116 non-null  float64\n",
            " 6   x_5     40116 non-null  float64\n",
            " 7   x_6     40116 non-null  float64\n",
            " 8   x_7     40116 non-null  float64\n",
            " 9   x_8     40116 non-null  float64\n",
            " 10  x_9     40116 non-null  float64\n",
            " 11  x_10    40116 non-null  float64\n",
            " 12  y       40116 non-null  float64\n",
            "dtypes: float64(12), object(1)\n",
            "memory usage: 4.3+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 40116 entries, 0 to 40117\n",
            "Data columns (total 13 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   ID      40116 non-null  object \n",
            " 1   x_0     40116 non-null  float64\n",
            " 2   x_1     40116 non-null  float64\n",
            " 3   x_2     40116 non-null  float64\n",
            " 4   x_3     40116 non-null  float64\n",
            " 5   x_4     40116 non-null  float64\n",
            " 6   x_5     40116 non-null  float64\n",
            " 7   x_6     40116 non-null  float64\n",
            " 8   x_7     40116 non-null  float64\n",
            " 9   x_8     40116 non-null  float64\n",
            " 10  x_9     40116 non-null  float64\n",
            " 11  x_10    40116 non-null  float64\n",
            " 12  y       40116 non-null  float64\n",
            "dtypes: float64(12), object(1)\n",
            "memory usage: 4.3+ MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-2e95c760ab3c>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[f'{col}'] = np.log(df[col] - min_value + 1)\n",
            "<ipython-input-32-2e95c760ab3c>:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data_cleaned[colums]=data_scaled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Load the data\n",
        "# # train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# # y 열을 기준으로 데이터프레임 전체를 내림차순 정렬\n",
        "# train_df_sorted = data_cleaned.sort_values(by='x_7', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# # Determine the number of columns and rows for the subplot grid\n",
        "# num_cols = 4\n",
        "# num_rows = 3\n",
        "\n",
        "# # Create the subplots\n",
        "# fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 15))\n",
        "\n",
        "# # Flatten the axes array for easy iteration\n",
        "# axes = axes.flatten()\n",
        "\n",
        "# # Plot each column as a time series, sorted by 'y'\n",
        "# for i, column in enumerate(train_df_sorted.columns[1:]):  # y 열을 제외한 나머지 열에 대해 반복\n",
        "#     if i < len(axes):\n",
        "#         # y 열을 기준으로 내림차순으로 정렬된 데이터에 대해 각 열을 플롯팅\n",
        "#         axes[i].plot(train_df_sorted.index, train_df_sorted[column], label=f\"{column} (sorted by y)\")\n",
        "#         axes[i].set_title(f\"{column} (sorted by y)\")\n",
        "#         axes[i].set_ylabel(column)\n",
        "#         axes[i].legend()\n",
        "\n",
        "# # Hide any unused subplots\n",
        "# for j in range(i + 1, len(axes)):\n",
        "#     fig.delaxes(axes[j])\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "pwKXJq9pje8Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "83VLTtAGdPHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf928601-19b0-4522-d788-1bef7b4221ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-6d51319af09c>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['pca_1'] = pca_result[:, 0]\n",
            "<ipython-input-34-6d51319af09c>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['pca_2'] = pca_result[:, 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "def PCA_df(df):\n",
        "  # 상관관계가 높은 특성들만 선택\n",
        "  features_to_pca = df[['x_7', 'x_8', 'x_9', 'x_10']]\n",
        "\n",
        "  # PCA 모델 생성 (2개의 주성분으로 차원 축소)\n",
        "  pca = PCA(n_components=2)\n",
        "  pca_result = pca.fit_transform(features_to_pca)\n",
        "\n",
        "  # PCA 결과를 새로운 컬럼으로 추가\n",
        "  df['pca_1'] = pca_result[:, 0]\n",
        "  df['pca_2'] = pca_result[:, 1]\n",
        "\n",
        "  # 변환 후 데이터 확인\n",
        "  # print(df[['pca_1', 'pca_2']].head())\n",
        "\n",
        "  #기존 특성을 drop해야하는지는 고민을 해봐야 함\n",
        "  result_df = df.drop(columns=['x_7', 'x_8', 'x_9', 'x_10'])\n",
        "\n",
        "  return result_df\n",
        "\n",
        "data_dropped=PCA_df(data_cleaned)\n",
        "test_df=PCA_df(test_df)\n",
        "# data_dropped=data_cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cols = [col for col in data_dropped.columns if col != 'y']\n",
        "\n",
        "# y 컬럼을 마지막으로 배치한 새로운 컬럼 순서\n",
        "data_dropped = data_dropped[cols + ['y']]\n",
        "\n",
        "# 결과 확인\n",
        "print(data_dropped.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PQEOn0pmJDT",
        "outputId": "e4ad04a7-127b-42fc-d6ca-1b85c9d262a6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            ID       x_0       x_1       x_2       x_3       x_4       x_5  \\\n",
            "0  TRAIN_00000 -1.023173  0.908064  1.218843  1.189377  0.302230 -0.304251   \n",
            "1  TRAIN_00001 -0.557117 -0.958310 -1.566851 -0.456552 -0.244841  0.479376   \n",
            "2  TRAIN_00002  0.373512 -0.862606 -0.397623  0.100983  0.503579 -0.634193   \n",
            "3  TRAIN_00003  1.013816 -0.895440 -0.452501 -1.189653  0.029569 -0.266155   \n",
            "4  TRAIN_00004 -0.590305  0.359653  1.082499  1.463757  0.333963 -0.371275   \n",
            "\n",
            "        x_6     pca_1     pca_2          y  \n",
            "0 -1.187358  1.200027 -0.583423  83.424500  \n",
            "1  1.002761 -2.010269  0.833411  79.374109  \n",
            "2  0.631756 -1.446934 -0.069400  82.181616  \n",
            "3  0.970894 -1.694061  0.201990  83.006586  \n",
            "4 -1.156152  1.078825 -0.544871  83.051434  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpZeJvc9mKqw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8m1W4U7RzHnB"
      },
      "outputs": [],
      "source": [
        "# Reduce Memory Usage\n",
        "# reference : https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 @ARJANGROEN\n",
        "\n",
        "def reduce_memory_usage(df):\n",
        "\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype.name\n",
        "        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n",
        "            if (col_type != 'object'):\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int8)\n",
        "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "\n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        pass\n",
        "            else:\n",
        "                df[col] = df[col].astype('category')\n",
        "    mem_usg = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage became: \",mem_usg,\" MB\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Y26WuiO8zHnD"
      },
      "outputs": [],
      "source": [
        "# train_df_org_reduced = reduce_memory_usage(data_dropped)\n",
        "# test_df_reduced = reduce_memory_usage(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "def top_10_percent_recall(y_true, y_pred):\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings('error', category=ConvergenceWarning)\n",
        "            # 예측을 수행하는 대신 직접 받은 예측값을 사용\n",
        "\n",
        "                        # 상위 10% 예측 및 상위 5% 실제값 기준 설정\n",
        "            top_10_percent_cutoff = np.percentile(y_pred, 90)\n",
        "            top_5_percent_cutoff = np.percentile(y_true, 95)\n",
        "            top_10_pred_indices = np.where(y_pred >= top_10_percent_cutoff)[0]\n",
        "            top_5_true_indices = np.where(y_true >= top_5_percent_cutoff)[0]\n",
        "\n",
        "            # 교집합 계산\n",
        "            common_indices = np.intersect1d(top_10_pred_indices, top_5_true_indices)\n",
        "\n",
        "            # # 디버깅용 출력 (상위 10% 예측, 상위 5% 실제 값, 교집합)\n",
        "            # print(f\"Top 10% Predicted Indices: {top_10_pred_indices}\")\n",
        "            # print(f\"Top 5% True Indices: {top_5_true_indices}\")\n",
        "            # print(f\"Common Indices (Intersection): {common_indices}\")\n",
        "\n",
        "            # 실제 상위 5% 중 예측 상위 10%에 포함된 비율 (Recall 계산)\n",
        "            if len(top_5_true_indices) == 0:\n",
        "                return 0  # 상위 5% 데이터가 없으면 Recall 0\n",
        "            recall = len(common_indices) / len(top_5_true_indices)\n",
        "\n",
        "            return recall\n",
        "    except Warning:\n",
        "        return -np.inf  # 경고 발생 시 매우 낮은 점수 반환\n",
        "\n",
        "\n",
        "# 사용자 정의 평가 함수로 스코어러 만들기\n",
        "custom_scorer = make_scorer(top_10_percent_recall, greater_is_better=True)"
      ],
      "metadata": {
        "id": "fx9qLVD2lPD8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dropped.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z14SpjhFuF2-",
        "outputId": "7bf280ad-8745-4e5a-f4db-83557033066b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 40116 entries, 0 to 40117\n",
            "Data columns (total 11 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   ID      40116 non-null  object \n",
            " 1   x_0     40116 non-null  float64\n",
            " 2   x_1     40116 non-null  float64\n",
            " 3   x_2     40116 non-null  float64\n",
            " 4   x_3     40116 non-null  float64\n",
            " 5   x_4     40116 non-null  float64\n",
            " 6   x_5     40116 non-null  float64\n",
            " 7   x_6     40116 non-null  float64\n",
            " 8   pca_1   40116 non-null  float64\n",
            " 9   pca_2   40116 non-null  float64\n",
            " 10  y       40116 non-null  float64\n",
            "dtypes: float64(10), object(1)\n",
            "memory usage: 3.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ygNl4YFuzHnF"
      },
      "outputs": [],
      "source": [
        "# df_shuffled= train_df_org\n",
        "df_shuffled= data_dropped\n",
        "\n",
        "# 균등하게 나누기 위해 임의로 데이터를 8:2로 분할\n",
        "split_index = int(len(df_shuffled) * 0.8)\n",
        "train_df, valiation_df = df_shuffled[:split_index], df_shuffled[split_index:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "F9qkR7G-zHnG"
      },
      "outputs": [],
      "source": [
        "X = train_df.iloc[:,1:-1]\n",
        "y = train_df['y']\n",
        "X_test=valiation_df.iloc[:,1:-1]\n",
        "y_test=valiation_df['y']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model = HuberRegressor()\n",
        "model.fit(X,y)\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "print(model,top_10_percent_recall(y_test, y_pred))\n",
        "print(mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xib8shVWo4nC",
        "outputId": "ea15b54a-95c5-42cc-cb4c-d7591808c4bd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuberRegressor() 0.8781094527363185\n",
            "2.526222026500817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "huber_params = {\n",
        "'epsilon': [1.1, 1.35, 1.5, 2.0],\n",
        "'alpha': np.logspace(-4, 1, 20),\n",
        "'fit_intercept': [True, False],\n",
        "'max_iter': [100, 500, 1000, 5000]\n",
        "}\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# 사용자 정의 평가 함수로 스코어러 만들기\n",
        "custom_scorer = make_scorer(top_10_percent_recall, greater_is_better=True)\n",
        "\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "model = HuberRegressor()\n",
        "\n",
        "\n",
        "# RandomizedSearchCV 설정\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=huber_params ,\n",
        "    n_iter=30,  # 시도할 하이퍼파라미터 조합의 수\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    scoring=custom_scorer,  # 평가 척도\n",
        "    random_state=42,  # 결과 재현을 위한 랜덤 시드\n",
        "    n_jobs=-1,  # 병렬처리\n",
        "    verbose=2  # 진행 상황 출력\n",
        ")\n",
        "\n",
        "# 모델 훈련 및 하이퍼파라미터 튜닝\n",
        "random_search.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "r-nmrcASOwc3",
        "outputId": "c0c0629c-a847-444e-eda7-bb95b8677dcc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, estimator=HuberRegressor(), n_iter=30, n_jobs=-1,\n",
              "                   param_distributions={'alpha': array([1.00000000e-04, 1.83298071e-04, 3.35981829e-04, 6.15848211e-04,\n",
              "       1.12883789e-03, 2.06913808e-03, 3.79269019e-03, 6.95192796e-03,\n",
              "       1.27427499e-02, 2.33572147e-02, 4.28133240e-02, 7.84759970e-02,\n",
              "       1.43844989e-01, 2.63665090e-01, 4.83293024e-01, 8.85866790e-01,\n",
              "       1.62377674e+00, 2.97635144e+00, 5.45559478e+00, 1.00000000e+01]),\n",
              "                                        'epsilon': [1.1, 1.35, 1.5, 2.0],\n",
              "                                        'fit_intercept': [True, False],\n",
              "                                        'max_iter': [100, 500, 1000, 5000]},\n",
              "                   random_state=42, scoring=make_scorer(top_10_percent_recall),\n",
              "                   verbose=2)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=HuberRegressor(), n_iter=30, n_jobs=-1,\n",
              "                   param_distributions={&#x27;alpha&#x27;: array([1.00000000e-04, 1.83298071e-04, 3.35981829e-04, 6.15848211e-04,\n",
              "       1.12883789e-03, 2.06913808e-03, 3.79269019e-03, 6.95192796e-03,\n",
              "       1.27427499e-02, 2.33572147e-02, 4.28133240e-02, 7.84759970e-02,\n",
              "       1.43844989e-01, 2.63665090e-01, 4.83293024e-01, 8.85866790e-01,\n",
              "       1.62377674e+00, 2.97635144e+00, 5.45559478e+00, 1.00000000e+01]),\n",
              "                                        &#x27;epsilon&#x27;: [1.1, 1.35, 1.5, 2.0],\n",
              "                                        &#x27;fit_intercept&#x27;: [True, False],\n",
              "                                        &#x27;max_iter&#x27;: [100, 500, 1000, 5000]},\n",
              "                   random_state=42, scoring=make_scorer(top_10_percent_recall),\n",
              "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3, estimator=HuberRegressor(), n_iter=30, n_jobs=-1,\n",
              "                   param_distributions={&#x27;alpha&#x27;: array([1.00000000e-04, 1.83298071e-04, 3.35981829e-04, 6.15848211e-04,\n",
              "       1.12883789e-03, 2.06913808e-03, 3.79269019e-03, 6.95192796e-03,\n",
              "       1.27427499e-02, 2.33572147e-02, 4.28133240e-02, 7.84759970e-02,\n",
              "       1.43844989e-01, 2.63665090e-01, 4.83293024e-01, 8.85866790e-01,\n",
              "       1.62377674e+00, 2.97635144e+00, 5.45559478e+00, 1.00000000e+01]),\n",
              "                                        &#x27;epsilon&#x27;: [1.1, 1.35, 1.5, 2.0],\n",
              "                                        &#x27;fit_intercept&#x27;: [True, False],\n",
              "                                        &#x27;max_iter&#x27;: [100, 500, 1000, 5000]},\n",
              "                   random_state=42, scoring=make_scorer(top_10_percent_recall),\n",
              "                   verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HuberRegressor</label><div class=\"sk-toggleable__content\"><pre>HuberRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HuberRegressor</label><div class=\"sk-toggleable__content\"><pre>HuberRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 최적의 하이퍼파라미터 조합 출력\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# 최적의 모델 출력\n",
        "best_model = random_search.best_estimator_\n",
        "print(\"Best Model:\", best_model)\n",
        "\n",
        "y_pred=best_model.predict(X_test)\n",
        "print(best_model,top_10_percent_recall(y_test, y_pred))\n",
        "print(mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0X1j5J_PgdI",
        "outputId": "9f61f3ad-a12e-40fc-d3a6-a86a89b2eb3e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_iter': 1000, 'fit_intercept': True, 'epsilon': 2.0, 'alpha': 2.9763514416313193}\n",
            "Best Model: HuberRegressor(alpha=2.9763514416313193, epsilon=2.0, max_iter=1000)\n",
            "HuberRegressor(alpha=2.9763514416313193, epsilon=2.0, max_iter=1000) 0.8805970149253731\n",
            "2.506765863618285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# org_train_df=pd.read_csv('/content/drive/MyDrive/dacon/2024_8_data/train.csv')\n",
        "# # df_shuffled= train_df_org\n",
        "# df_shuffled= org_train_df\n",
        "\n",
        "# # 균등하게 나누기 위해 임의로 데이터를 8:2로 분할\n",
        "# split_index = int(len(df_shuffled) * 0.8)\n",
        "# train_df, valiation_df = df_shuffled[:split_index], df_shuffled[split_index:]\n",
        "\n",
        "# X = train_df.iloc[:,1:-1]\n",
        "# y = train_df['y']\n",
        "# X_test=valiation_df.iloc[:,1:-1]\n",
        "# y_test=valiation_df['y']\n",
        "\n",
        "# model2 = HuberRegressor()\n",
        "# model2.fit(X,y)\n",
        "\n",
        "# y_pred=model2.predict(X_test)\n",
        "# print(model2,top_10_percent_recall(y_test, y_pred))\n",
        "# print(mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "eM7aBa9lphTw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# 모델 저장\n",
        "joblib.dump(best_model, '/content/drive/MyDrive/dacon/2024_8_data/PCA2_Fe_huber.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCZs3ah-QeDG",
        "outputId": "b921a12b-8df6-499f-d442-c1ae59414c1b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/dacon/2024_8_data/PCA2_Fe_huber.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Predict on test data\n",
        "X_test = test_df.iloc[:, 1:]\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Identify top 5% of predicted values\n",
        "threshold = np.percentile(y_pred, 95)\n",
        "top_5_percent_mask = y_pred >= threshold\n",
        "\n",
        "# Create submission file\n",
        "submission_df = pd.read_csv('/content/drive/MyDrive/dacon/2024_8_data/sample_submission.csv')\n",
        "submission_df['y'] = y_pred\n",
        "submission_df.to_csv('/content/drive/MyDrive/dacon/2024_8_data/PCA2_Fe_huber.csv', index=False)\n",
        "\n",
        "print(f\"Top 5% threshold: {threshold:.4f}\")\n",
        "print(f\"Number of samples in top 5%: {sum(top_5_percent_mask)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueFnEra-wAed",
        "outputId": "e5bafec9-7a4e-4119-d613-161ab1ae2e66"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5% threshold: 91.2746\n",
            "Number of samples in top 5%: 250\n"
          ]
        }
      ]
    }
  ]
}